{"backend_state":"ready","connection_file":"/tmp/xdg-runtime-user/jupyter/kernel-aebaef2e-a470-4160-a041-f6d27edd2b77.json","kernel":"julia-1.8","kernel_error":"","kernel_state":"idle","kernel_usage":{"cpu":0,"memory":0},"last_ipynb_save":1714253599362,"metadata":{"language_info":{"file_extension":".jl","mimetype":"application/julia","name":"julia","version":"1.8.4"}},"trust":true,"type":"settings"}
{"cell_type":"code","exec_count":0,"id":"01dcb7","input":"example = [1, 2, 5, 7, 8, 11, 13, 14]\n\nK = 3;","pos":29,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"03c7a9","input":"# Let's try for K = 4 on the same dataset\n\nkmeanscluster(example, 4)","pos":40,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"03eba5","input":"clusters = clustering(example, centers)","pos":34,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"0af545","input":"q1 = \"\"","pos":45,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"0eb664","input":"# First, we want to randomly choose K center points\n\nfunction center_points(data, K)\n    if length(data) < K\n        K = length(data)\n    end\n\n    while true\n        output = data[rand(1:length(data), K)]\n        if length(unique(output)) == K\n            return sort(output)\n        end\n    end\nend","pos":30,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"148980","input":"# Then, we'll cluster each datapoint according to their distance to the center points\n\nfunction clustering(data, centers)\n    K = length(centers)\n    output = []\n\n    # Initialize clusters\n    for i = 1:K\n        push!(output, [])\n    end\n\n    # cluster each datapoint based on miminum distance\n    for i = 1:length(data)\n        xi_dist = distance(data[i], centers)\n        idx = findfirst(xi_dist .== minimum(xi_dist))\n        push!(output[idx], data[i])\n        sort!(output[idx])\n    end\n\n    return output\nend","pos":33,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"177fd6","input":"q2 = \"\"","pos":48,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"18a67c","input":"","pos":115,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"1e6519","input":"","pos":98,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2b7ae8","input":"","pos":66,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"2e568d","input":"function clustering(data, centers)\n    # YOUR SOLUTION HERE\nend","pos":120,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"343e88","input":"function score(data, K)\n    result = kmeans(data, K)\n\n    a = assignments(result)\n    centers = result.centers;\n\n    var_values = zeros(K);  # this will store our variance values\n    for i = 1:K\n        temp = data[a .== i]\n        var_values[i] = var(temp)\n    end\n\n    return sum(var_values)\nend","pos":63,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"35a9f4","input":"# Let's do another example dataset\n\nexample = [1 2 3 6 7 8 15 16 17]\n\n\n@show score(example, 1);\n@show score(example, 2);\n@show score(example, 3);\n@show score(example, 4);\n@show score(example, 5);","pos":65,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"361eef","input":"centers = center_points(example, K)","pos":31,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"365a82","input":"","pos":127,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"395bc3","input":"","pos":86,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"3b28eb","input":"","pos":118,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"454502","input":"q2 = \"\"  # Type your answer in the quotation marks","pos":88,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4a72c3","input":"","pos":136,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"4d8bb0","input":"# Next, we'll calculate the mean of each clusters, which will become our new center points\n\nfunction clustermeans(clusters)\n    K = size(clusters, 1)\n    output = zeros(K)\n    for i = 1:K\n        output[i] = mean(clusters[i])\n    end\n    return output\nend","pos":35,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"51365e","input":"group1 = \"[]\"\ngroup2 = \"[]\"\ngroup3 = \"[]\"","pos":51,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"520def","input":"","pos":89,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"55b7b2","input":"","pos":110,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5a7efb","input":"","pos":42,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5ac3ac","input":"example = [1 2 5 7 8 11 13 14]\n\n@show score(example, 1);  # Note that K=1 is considered the worst case scenario\n@show score(example, 2);\n@show score(example, 3);\n@show score(example, 4);","pos":64,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5c7d58","input":"function greatest_center(data, K)\n    result = kmeans(data, K)\n    return maximum(result.centers)\nend","pos":130,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"5f7451","input":"function centermean(data, K)\n    result = kmeans(data, K)\n    return mean(result.centers)\nend","pos":132,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"626659","input":"function centerpoints(data, K)\n    # YOUR SOLUTION HERE\nend","pos":114,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7573a5","input":"","pos":124,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7745a2","input":"","pos":5,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"7a57d5","input":"","pos":107,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"827167","input":"function least_center(data, K)\n    # YOUR SOLUTION HERE\nend","pos":106,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"87f132","input":"kmeanscluster(example, 3)","pos":39,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"973f20","input":"# We define the function distance to measure the distance between the datapoint and each center points\n\nfunction distance(x, centers)\n    output = zeros(length(centers))\n    for i = 1:length(output)\n        output[i] = abs(x - centers[i])\n    end\n    return output\nend","pos":32,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a560c8","input":"","pos":46,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a5e1a9","input":"","pos":133,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"a8713c","input":"function centermean(data, K)\n    # YOUR SOLUTION HERE\nend","pos":109,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ab9168","input":"function greatest_center(data, K)\n    # YOUR SOLUTION HERE\nend","pos":103,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"abcd67","input":"","pos":49,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"ac1a07","input":"function centerpoints(data, K)\n    return shuffle(data)[1:K]\nend\n\n\nfunction Euclidean_distance(point, centers)\n    output = zeros(length(centers))\n    for i = 1:length(output)\n        output[i] = sqrt((point[1]-centers[i][1])^2 + (point[2]-centers[i][2])^2)\n    end\n    return output\nend\n\n\nfunction cluster(data, centers)\n    K = length(centers)\n    output = []\n\n    for i = 1:K\n        push!(output, [])\n    end\n\n    for i = 1:length(data)\n        dist = Euclidean_distance(data[i], centers)\n        idx = findfirst(dist .== minimum(dist))\n        push!(output[idx], data[i])\n        sort!(output[idx])\n    end\n\n    return output\nend\n\n\nfunction clustermeans(clusters)\n    K = length(clusters)\n    output = []\n    for i = 1:K\n        push!(output, mean(clusters[i]))\n    end\n    return output\nend\n\n\nfunction kmeans2D(data, K)\n    centers = centerpoints(data, K)\n\n    init_clusters = cluster(data, centers)\n\n    mean_vals = clustermeans(init_clusters)\n\n    prev = init_clusters\n    centers = mean_vals\n\n    while true\n        output = cluster(data, centers)\n        if all(output .== prev)\n            return output\n        else\n            prev = output\n            centers = clustermeans(output)\n        end\n    end\nend","pos":135,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b053ce","input":"function kmeans2D(data, K)\n    # YOUR SOLUTION HERE\nend","pos":126,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b0a777","input":"","pos":75,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b5264b","input":"# Using the built-in K-Means Clustering Function\n\nexample = [1 2 5 7 8 11 13 14]\n\nresult = kmeans(example, K)\n\n@show a = assignments(result);   # get the assignment of points to clusters\n@show c = counts(result);        # get the cluster sizes\n@show centers = result.centers;  # get the cluster centers","pos":41,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"b52f7f","input":"function clustermeans(clusters)\n    # YOUR SOLUTION HERE\nend","pos":123,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"bdb1e5","input":"# Let's put them all together into one function:\n\nfunction kmeanscluster(data, K)\n    # Randomly select center points c_1, ..., c_K\n    centers = center_points(data, K)\n\n\n    # For each datapoint, find the minimum distance and cluster\n    init_clusters = clustering(data, centers)\n\n\n    # Calculate the mean of each clusters\n    mean_vals = clustermeans(init_clusters)\n\n\n    # Repeat clustering process until stability is reached\n    prev = init_clusters\n    centers = mean_vals\n\n    while true\n        output = clustering(data, centers)\n        if all(output .== prev)\n            return output\n        else\n            prev = output\n            centers = clustermeans(output)\n        end\n    end\nend\n\n# NOTE: This function is written for only the 1-dimensional case","pos":38,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"be3600","input":"meanvals = clustermeans(clusters)","pos":36,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c24e20","input":"","pos":121,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"c70e6b","input":"q1 = \"\"  # Type your answer in the quotation marks","pos":85,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"cbe5ee","input":"","pos":92,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"dd4a37","input":"# We will repeat the process of clustering using the mean values until we reach stability\n\nprev = clusters\ncenters = meanvals\n\nwhile true\n    output = clustering(example, centers)\n    if all(output .== prev)\n        return output\n    else\n        prev = output\n        centers = clustermeans(output)\n    end\nend\n\noutput","pos":37,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e3474a","input":"","pos":81,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e4919f","input":"function least_center(data, K)\n    result = kmeans(data, K)\n    return minimum(result.centers)\nend","pos":131,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e7995e","input":"","pos":52,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"e97844","input":"function Euclidean_distance(point, centers)\n    # YOUR SOLUTION HERE\nend","pos":117,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"eef0c2","input":"q3 = \"\"  # Type your answer in the quotation marks","pos":91,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"f823e2","input":"","pos":104,"type":"cell"}
{"cell_type":"code","exec_count":0,"id":"fc4a7d","input":"","pos":23,"type":"cell"}
{"cell_type":"code","exec_count":5,"id":"8b030d","input":"using Pkg\n\nPkg.add(\"Clustering\")  # run this if you don't have Clustering\n\nusing DataFrames\nusing Clustering\nusing Statistics\nusing Random","pos":4,"type":"cell"}
{"cell_type":"markdown","id":"00db3b","input":"### Question 3:\n\nSuppose we have the following data set: and we want to cluster it into 3 groups.\n\nWhat groups should the K-Means Clustering Algorithm ideally return?\n\n<img src=\"/images/participation1.png\" style=\"max-width:100%\" />","pos":50,"type":"cell"}
{"cell_type":"markdown","id":"045ae6","input":"Turns out, K-Means Clustering can't \"see\" the best clustering for a given value of $K$.\n\nIts only option is to keep track of these clusters for different starting center points.\n","pos":59,"type":"cell"}
{"cell_type":"markdown","id":"1c2012","input":"---","pos":82,"type":"cell"}
{"cell_type":"markdown","id":"1cd3a6","input":"# Run this first","pos":3,"type":"cell"}
{"cell_type":"markdown","id":"1de47c","input":"Similarly, we can treat a heatmap with $n$ columns the same as an $n$-dimensional graph when we run K-Means Clustering.\n\nNote that we don't actually need to plot our data points in order to cluster it; we just need to calculate the distances.\n\nIn general, if a heatmap has $n$ samples or axes, then the Euclidean distance is:\n\n$$\n distance = \\sqrt{x_1^2 + x_2^2 + ... + x_n^2}\n$$","pos":80,"type":"cell"}
{"cell_type":"markdown","id":"1e69bc","input":"<img src=\"/images/intro.png\" style=\"max-width:100%\" />","pos":9,"type":"cell"}
{"cell_type":"markdown","id":"21370b","input":"## Solutions","pos":128,"type":"cell"}
{"cell_type":"markdown","id":"240f13","input":"Write a function `centermean`, which takes in as input a dataset and a value $K$, and outputs the mean of the center values.","pos":108,"type":"cell"}
{"cell_type":"markdown","id":"2502f8","input":"Write a function clustermeans, which takes as input the clusters obtained from clustering, and outputs a vector containing the mean of each clusters.\n\nHint: use the built in mean function.","pos":122,"type":"cell"}
{"cell_type":"markdown","id":"298de5","input":"---","pos":2,"type":"cell"}
{"cell_type":"markdown","id":"29e888","input":"<img src=\"/images/step3.png\" style=\"max-width:100%\" />","pos":16,"type":"cell"}
{"cell_type":"markdown","id":"2ae4d9","input":"There is actually a built-in [K-Means Clustering](https://github.com/JuliaStats/Clustering.jl) function in the JuliaStats library, and we will eventually use it, but for now we will implement K-Means Clustering ourselves (for the 1-Dimensional case).","pos":26,"type":"cell"}
{"cell_type":"markdown","id":"2e68d1","input":"In our demo with the straight line, we've gotten \"lucky\" with getting the correct clusters. \n\nHowever, this won't always be the case; the output clusters can vary depending on what $K$ center points the computer randomly selects.","pos":55,"type":"cell"}
{"cell_type":"markdown","id":"308d41","input":"There are so many images on the internet, that it became necessary to develop a method to compress images into a smaller size file without compromising the image quality to an unacceptable level. This allows more images to be stored into a hard drive, file, etc., as well as reducing the amount of time required to send an image (via email or on the Internet etc..).\n\nImages are represented using pixels, which are composed of Red, Green, Blue values (RGB), each of which ranges from 0-255 to indicate the \"amount\" of red, green, blue respectively.\n\nK-Means Clustering takes advantage of the fact that different combinations of RGB values looks the same to the human eye. \n\nFor example, an RGB value of $(32, 171, 69)$ looks very similar to $(30, 170, 65)$ to the human eye.","pos":95,"type":"cell"}
{"cell_type":"markdown","id":"31128e","input":"# **Measuring the Success of K-Means Clustering**","pos":54,"type":"cell"}
{"cell_type":"markdown","id":"33789c","input":"### Question 1:\n\n**Multiple Choice**: Given the following heatmap, which of the following 2D plots best represents the datapoints?\n\n<img src=\"/images/participation2_1.png\" style=\"max-width:100%\" />\n\n<img src=\"/images/participation2_2.png\" style=\"max-width:100%\" />","pos":84,"type":"cell"}
{"cell_type":"markdown","id":"33be18","input":"---\n# **Applications: Image Compression**","pos":93,"type":"cell"}
{"cell_type":"markdown","id":"3c8fa7","input":"<img src=\"/images/step4.png\" style=\"max-width:100%\" />","pos":17,"type":"cell"}
{"cell_type":"markdown","id":"40941a","input":"# Contents\n\n- [Introduction to Clustering](https://cocalc.com/projects/03ebcf04-2031-4476-a136-16bd4155727a/files/Assignments/Final%20Project%20Submission/Final%20Project.ipynb#Introduction-to-Clustering)\n- [K-Means Clustering](https://cocalc.com/projects/03ebcf04-2031-4476-a136-16bd4155727a/files/Assignments/Final%20Project%20Submission/Final%20Project.ipynb#K-Means-Clustering)\n- [K-Means Clustering in Julia](https://cocalc.com/projects/03ebcf04-2031-4476-a136-16bd4155727a/files/Assignments/Final%20Project%20Submission/Final%20Project.ipynb#K-Means-Clustering-in-Julia)\n- [Participation Check 1](https://cocalc.com/projects/03ebcf04-2031-4476-a136-16bd4155727a/files/Assignments/Final%20Project%20Submission/Final%20Project.ipynb#Participation-Check)\n- [Measuring the Success of K-Means Clustering](https://cocalc.com/projects/03ebcf04-2031-4476-a136-16bd4155727a/files/Assignments/Final%20Project%20Submission/Final%20Project.ipynb#Measuring-the-Success-of-K-Means-Clustering)\n- [K-Means Clustering in other cases](https://cocalc.com/projects/03ebcf04-2031-4476-a136-16bd4155727a/files/Assignments/Final%20Project%20Submission/Final%20Project.ipynb#K-Means-Clustering-for-other-cases)\n- [Participation Check 2](https://cocalc.com/projects/03ebcf04-2031-4476-a136-16bd4155727a/files/Assignments/Final%20Project%20Submission/Final%20Project.ipynb#Participation-Check-2)\n- [Applications: Image Compression](https://cocalc.com/projects/03ebcf04-2031-4476-a136-16bd4155727a/files/Assignments/Final%20Project%20Submission/Final%20Project.ipynb#Applications:-Image-Compression)\n- [Mini Homework](https://cocalc.com/projects/03ebcf04-2031-4476-a136-16bd4155727a/files/Assignments/Final%20Project%20Submission/Final%20Project.ipynb#Mini-Homework)","pos":1,"type":"cell"}
{"cell_type":"markdown","id":"423e11","input":"Write a function `Euclidean_distance`, which takes in as input a datapoint with two elements and 2D array of center points, and outputs the Euclidean distance from the datapoint to each of the center points. You may assume that all of the entries in centers and the datapoint have two elements.","pos":116,"type":"cell"}
{"cell_type":"markdown","id":"4a0277","input":"Write a function `centerpoints` with takes in as input a 2D dataset (a vector of vectors containing 2 elements) and a value of $K$ and returns $K$ randomly selected center points. The center points must be unique.\nYou may use the shuffle function.","pos":113,"type":"cell"}
{"cell_type":"markdown","id":"4a3019","input":"The difference is that for computing the distance, we use the **Euclidean Distance** (or Pythagorean's Theorem) to compute the distance for each points from the center points.\n\n$$\n distance = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}\n$$","pos":71,"type":"cell"}
{"cell_type":"markdown","id":"51d563","input":"Since the human eye cannot perceive the difference between RGB values that are very close or similar to each other, it is a good idea to group them instead.\n\nThe way this works is to use K-Means Clustering to group all the colors in an image into $K$ groups of similar colors (for example, $K = 64$). The center points will be representative of groups of similar colors. Then we will replace every pixels with their respective center points in the final output image.\n\nFor example, for our above RGB values $(32, 171, 69)$ and $(30, 170, 65)$: if we run the algorithm and their group's center value happens to be something like $(31, 170, 67)$, then all of the pixels containing these original values will be replaced by $(31, 170, 67)$ in the output image.\n\nThus, the total color combination using only $K$ colors will be significantly less than the total color combinations of RGB values (which is $256^3$, or $16, 777, 216$), which allows us to save significant amount of space and store more images in a file, hard drive, etc..","pos":97,"type":"cell"}
{"cell_type":"markdown","id":"536bb0","input":"Suppose you have data represented on a heat map:","pos":77,"type":"cell"}
{"cell_type":"markdown","id":"547607","input":"<img src=\"/images/2d_demo.gif\" style=\"max-width:100%\" />","pos":73,"type":"cell"}
{"cell_type":"markdown","id":"662767","input":"### Question 2:\n\nIf we were to run K-Means Algorithm on this heatmap, what should our value of $K$ be?","pos":87,"type":"cell"}
{"cell_type":"markdown","id":"6873ae","input":"<img src=\"/images/step5.png\" style=\"max-width:100%\" />","pos":18,"type":"cell"}
{"cell_type":"markdown","id":"6949f6","input":"# **Introduction to Clustering**","pos":7,"type":"cell"}
{"cell_type":"markdown","id":"69db8a","input":"<img src=\"/images/step1.png\" style=\"max-width:100%\" />","pos":14,"type":"cell"}
{"cell_type":"markdown","id":"6a4a4a","input":"Using the previous functions, implement a function `kmeans2D`, which takes as input a 2D dataset and the $K$ value, and outputs a vector containing the clusters.","pos":125,"type":"cell"}
{"cell_type":"markdown","id":"6bafc5","input":"Suppose you have a data that is represented on a 2D graph, or an $xy$-graph.\n\n","pos":69,"type":"cell"}
{"cell_type":"markdown","id":"6e81c8","input":"---","pos":53,"type":"cell"}
{"cell_type":"markdown","id":"6ee854","input":"# **Mini Homework**","pos":100,"type":"cell"}
{"cell_type":"markdown","id":"742a48","input":"Suppose you have data set that can be plotted on a straight line, an $xy$ graph, or even a heatmap.","pos":8,"type":"cell"}
{"cell_type":"markdown","id":"7c6d4e","input":"<img src=\"/images/step8_2.png\" style=\"max-width:100%\" />","pos":22,"type":"cell"}
{"cell_type":"markdown","id":"7f286f","input":"### 2D-Graph","pos":68,"type":"cell"}
{"cell_type":"markdown","id":"830b32","input":"There are many different types of clustering algorithms in classification problems, such as Mean-Shift Clustering, Density-Based Spatial Clustering of Applications with Noise (DBSCAN), Hierarchical Clustering, (and even) K-Medians Clustering, etc..\n\nThe one we will be focusing on is **K-Means Clustering**.","pos":11,"type":"cell"}
{"cell_type":"markdown","id":"836878","input":"Write a function `clustering`, which takes as input a 2D dataset and a vector containing $K$ center points, and clusters the data into $K$ groups depending on the Euclidean distance to the nearest center point.","pos":119,"type":"cell"}
{"cell_type":"markdown","id":"857a65","input":"---","pos":24,"type":"cell"}
{"cell_type":"markdown","id":"89f3e3","input":"<img src=\"/images/success1.png\" style=\"max-width:100%\" />","pos":57,"type":"cell"}
{"cell_type":"markdown","id":"8cf66b","input":"### Problem 2","pos":134,"type":"cell"}
{"cell_type":"markdown","id":"94735e","input":"# **Participation Check 2**\n\n","pos":83,"type":"cell"}
{"cell_type":"markdown","id":"9589e5","input":"# **K-Means Clustering in Julia**","pos":25,"type":"cell"}
{"cell_type":"markdown","id":"9c4f58","input":"Similarly, write a function `least_center`, which takes in as input a (1D) dataset and a value $K$, and outputs the smallest center value.","pos":105,"type":"cell"}
{"cell_type":"markdown","id":"9e9ac7","input":"---\n# **Participation Check**","pos":43,"type":"cell"}
{"cell_type":"markdown","id":"a154c3","input":"Other than making your data look cleaner and more organized, K-Means clustering has many applications.\n\nOne of them is **[image compression](https://www.geeksforgeeks.org/image-compression-using-k-means-clustering/)**.","pos":94,"type":"cell"}
{"cell_type":"markdown","id":"a8ebdd","input":"With these datasets, we want to put them into different groups.\n\nThis is the motivation for clustering algorithms.\n\n**Clustering** is a Machine Learning technique, referring to grouping data points into different \"classes\" or categories using the help of a computer. It is used very frequently in data science and data cleaning.\n\nFor the sample datasets in the picture above, we can simply do it with our own human eyes and calculation, but it can get very tedious when we have a large dataset, and often times it might not be exactly clear which group a certain datapoint should be categorized into.\nInstead, we should rely on the computer to do the hard work for us.","pos":10,"type":"cell"}
{"cell_type":"markdown","id":"a9503d","input":"Just like we did in the straight-line case, we identify $K$, and then randomly select $K$ center points.","pos":70,"type":"cell"}
{"cell_type":"markdown","id":"a9e0ae","input":"$$\\text{Variance} = \\frac{1}{n} \\sum_{i=1}^{n} || a_i - \\mu||^2 $$\n\n- $\\mu$ = mean value\n- n = number of datapoints","pos":61,"type":"cell"}
{"cell_type":"markdown","id":"aa11e8","input":"We then measure the \"score\" of a particular clustering by calculating the **Within-Cluster Sum of Squares** (WCSS):\n\n$$\n\\text{score(Clustering)} = \\sum_{\\text{clusters}} \\text{Variance(cluster)}\n$$\n\nWe want the WCSS for a particular cluster (or particular value of K) to be small.","pos":62,"type":"cell"}
{"cell_type":"markdown","id":"accb46","input":"If we run the algorithm, we would get this as our output:\n\n<img src=\"/images/success2.png\" style=\"max-width:100%\" />\n\nIs this \"correct\"?","pos":58,"type":"cell"}
{"cell_type":"markdown","id":"acdf80","input":"### Heatmap","pos":76,"type":"cell"}
{"cell_type":"markdown","id":"b37ce7","input":"### Question 3:\n\n**True/False**: K-Means Clustering will always guarantee the \"correct\" clusters, regardless of what center points are selected.","pos":90,"type":"cell"}
{"cell_type":"markdown","id":"b65b05","input":"Using the built-in functions, write a function `greatest_center` which takes in as input a dataset (you may assume it is a 1-Dimensional dataset) and a value $K$, and outputs the highest value among the center points.","pos":102,"type":"cell"}
{"cell_type":"markdown","id":"bded39","input":"<img src=\"/images/step2.png\" style=\"max-width:100%\" />","pos":15,"type":"cell"}
{"cell_type":"markdown","id":"c2d634","input":"In the following exercise, you will implement K-Means Clustering from scratch like we did in lecture, but for a 2D case. You may not use the built-in K-Means Clustering library and functions.","pos":112,"type":"cell"}
{"cell_type":"markdown","id":"c6fa0a","input":"# **K-Means Clustering**","pos":12,"type":"cell"}
{"cell_type":"markdown","id":"c7aba9","input":"We will consider the following example data that can be represented on a straight line:\n$$[1, 2, 5, 7, 8, 11, 13, 14]$$\n\nSuppose we want to cluster this into 3 groups.","pos":28,"type":"cell"}
{"cell_type":"markdown","id":"d406cd","input":"The way we can measure the success of K-Means Clustering is to use variance to measure the \"goodness\" of a particular cluster.\n\nRecap: **Variance** is basically a measure of how far each datapoints are from the mean value. A small variance means the datapoints are closer to the mean value, while a large variance means the points are far apart.","pos":60,"type":"cell"}
{"cell_type":"markdown","id":"d77f3d","input":"---","pos":99,"type":"cell"}
{"cell_type":"markdown","id":"d7bdf6","input":"<img src=\"/images/step7.png\" style=\"max-width:100%\" />","pos":20,"type":"cell"}
{"cell_type":"markdown","id":"d838bc","input":"---\n# **K-Means Clustering for other cases**","pos":67,"type":"cell"}
{"cell_type":"markdown","id":"dc6f5d","input":"### Problem 1","pos":129,"type":"cell"}
{"cell_type":"markdown","id":"e1aaf6","input":"To compute the mean point of a cluster, sum all the $x$ and the $y$ values separately, then divide both of them by the total number of points in that cluster.\n$$\n(\\frac{x_1 + ... + x_n}{n}, \\frac{y_1 + ... + y_n}{n})\n$$","pos":72,"type":"cell"}
{"cell_type":"markdown","id":"e1adfa","input":"# **FINAL PROJECT - K-Means Clustering**\n\n## Joseph Lee\n\n### Math 157\n","pos":0,"type":"cell"}
{"cell_type":"markdown","id":"e27a5d","input":"For example, suppose for this dataset the algorithm randomly selected these points to be our centroids:","pos":56,"type":"cell"}
{"cell_type":"markdown","id":"e63afd","input":"<img src=\"/images/step6.png\" style=\"max-width:100%\" />","pos":19,"type":"cell"}
{"cell_type":"markdown","id":"e941c3","input":"---","pos":6,"type":"cell"}
{"cell_type":"markdown","id":"eb05d8","input":"#### Pseudocode for K-Means Clustering:\n\n<img src=\"/images/pseudocode.png\" style=\"max-width:100%\" />","pos":27,"type":"cell"}
{"cell_type":"markdown","id":"eb44e9","input":"**[K-Means Clustering](https://www.youtube.com/watch?v=4b5d3muPQmA&ab_channel=StatQuestwithJoshStarmer)** is a machine learning algorithm that takes points in a dataset and groups them into one of $K$ groups according to their distance to the mean values.\n\nHere's how the algorithm works:","pos":13,"type":"cell"}
{"cell_type":"markdown","id":"ed5e0e","input":"Example with a 2D graph:\n\n[Wikipedia](https://en.wikipedia.org/wiki/File:K-means_convergence.gif)","pos":74,"type":"cell"}
{"cell_type":"markdown","id":"f00077","input":"<img src=\"/images/color.png\" style=\"max-width:100%\" />","pos":96,"type":"cell"}
{"cell_type":"markdown","id":"f0399d","input":"### Question 1:\n\n**True/False**: The $K$ in K-Means Clustering represents the number of groups that we want to cluster the data into.","pos":44,"type":"cell"}
{"cell_type":"markdown","id":"f1c3d0","input":"We can treat the columns as variables $x$ and $y$. Then we can plot the points onto a 2D graph and do the algorithm as we normally would for a 2D graph.\n\n<img src=\"/images/heatmap_demo.png\" style=\"max-width:100%\" />","pos":79,"type":"cell"}
{"cell_type":"markdown","id":"f47153","input":"<img src=\"/images/heatmap.png\" style=\"max-width:100%\" />","pos":78,"type":"cell"}
{"cell_type":"markdown","id":"f5cbc7","input":"### Question 2:\n\n**True/False**: After the initial clustering by using center points, you should then calculate the standard deviation of each clusters and cluster according to the nearest standard deviation.","pos":47,"type":"cell"}
{"cell_type":"markdown","id":"f7e965","input":"## Problem 2: Implementing for a 2D case","pos":111,"type":"cell"}
{"cell_type":"markdown","id":"f91ce4","input":"## Problem 1: Fun with built-in K-Means Clustering","pos":101,"type":"cell"}
{"cell_type":"markdown","id":"fa7e76","input":"<img src=\"/images/step8_1.png\" style=\"max-width:100%\" />","pos":21,"type":"cell"}
{"id":0,"time":1714177805519,"type":"user"}
{"last_load":1699752880655,"type":"file"}